{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import wandb\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from src import read_nz_file, read_jg_file, update_meta_data, split_df, aggregate_files, add_moving_window\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed data - intermediate step\n",
    "\n",
    "> raw data\n",
    "\n",
    "> clean data\n",
    "\n",
    "> preprocess data : store it in DB (better compared to file format) \n",
    "\n",
    "    - data lake\n",
    "    \n",
    "    or\n",
    "    \n",
    "    - DB model based SQL (Nice to have but not required if we deceide to save the data as a Feather file)\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and clean raw data\n",
    "\n",
    "files from SensorLog iOS app has in total over 70 colums and precision of 12 decimal figures. The output file is over 135 MB, which is too large for GitHub. GitHub restricts the file size, therefore these files from SensorLog must be cleaned.\n",
    "\n",
    "Below summary of steps which is done only for iOS files:\n",
    "\n",
    "- read raw data as csv files\n",
    "- remove unnecessary columns (captured in list 'remove_cols' below)\n",
    "- round to 6 decimal places to reduce the size of files\n",
    "- output dataframe as csv\n",
    "- upload the csv on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Meta data\n",
    "\n",
    "The data on different data files is captured in meta dataframe below:\n",
    "- file name\n",
    "- user (nz or jg)\n",
    "- activity (running/cycling/walking/sitting)\n",
    "- pocket (in which pocket handy was during the activity)\n",
    "- position_x\n",
    "- position_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. View data\n",
    "\n",
    "Sensor activity data is captured from 2 different Apps:\n",
    "- SensorLog (iOS) by user 'nz'\n",
    "- AndrioSensor (Andriod) by user 'jg'\n",
    "\n",
    "**Response**: 'Acivity' with 4 classes: running/walking/cycling/sitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_meta_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstn():\n",
    "    num = 0\n",
    "    while True:\n",
    "        yield num\n",
    "        num += 1\n",
    "\n",
    "k = firstn()\n",
    "l = [next(k) for i in range(10)]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('data/meta.csv')\n",
    "meta = meta[::3]\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects np array as input\n",
    "def one_hot_encode(l):\n",
    "    for i, v in enumerate(l):\n",
    "        if v == 'sitting':\n",
    "            l[i] = 0\n",
    "        elif v == 'walking':\n",
    "            l[i] = 1\n",
    "        elif v == 'running':\n",
    "            l[i] = 2\n",
    "        elif v == 'cycling':\n",
    "            l[i] = 3\n",
    "\n",
    "    l = to_categorical(np.array(l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an aggregate of all files and output a list with the possible start positions within the dataframe. (lookback must always be contained within a single activity)\n",
    "# saves this preprocessed file and list to the tmp folder\n",
    "def preprocess_sequential_2(moving_window_seconds, hz, step_size, test_proportion = 0.2):\n",
    "\n",
    "    # create empty data frames\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    # create empty lists of start indexes\n",
    "    train_indexes = []\n",
    "    test_indexes = []\n",
    "\n",
    "    for file, user, activity in zip(meta['file'], meta['user'], meta['activity']):\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "            df = df.drop(columns=['datetime'])\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, df.shape)\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds\n",
    "        )\n",
    "\n",
    "        # aggregate data points (try moving average) transform to mean, sd, ...\n",
    "        for i, (v_train, v_test) in enumerate(zip(my_train_files, my_test_files)):\n",
    "\n",
    "            # save possible start indexes for training and test sequences\n",
    "            lookback = hz * moving_window_seconds\n",
    "            max_train_index = len(v_train) - 1\n",
    "            max_test_index = len(v_test) - 1\n",
    "            train_rows = np.arange(lookback, max_train_index, step_size)\n",
    "            test_rows = np.arange(lookback, max_test_index, step_size)\n",
    "\n",
    "            # append start indexes of sequences to total lists\n",
    "            train_indexes += list(train_rows + len(train))\n",
    "            test_indexes += list(test_rows + len(test))\n",
    "\n",
    "            # append files to total file\n",
    "            train = pd.concat([train, v_train])\n",
    "            test = pd.concat([test, v_test])\n",
    "\n",
    "    # split x and y\n",
    "    y_train = train['y']\n",
    "    x_train = train.drop(columns=['y', 'time_since_start(ms)'])\n",
    "    y_test = test['y']\n",
    "    x_test = test.drop(columns=['y', 'time_since_start(ms)'])\n",
    "\n",
    "    # normalize data by training aggregates\n",
    "    mean = x_train.mean(axis=0)\n",
    "    x_train -= mean\n",
    "    x_test -= mean\n",
    "    std = x_train.std(axis=0)\n",
    "    x_train /= std\n",
    "    x_test /= std\n",
    "\n",
    "    # one hot encode labels\n",
    "    y_train = one_hot_encode(np.array(y_train))\n",
    "    y_test = one_hot_encode(np.array(y_test))\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, train_indexes, test_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sequential_preprocessing(\n",
    "        X_train, X_test, y_train, y_test, train_indexes, test_indexes, folder: str, settings: str):\n",
    "\n",
    "    if not os.path.exists(f'./tmp/{folder}'):\n",
    "        os.mkdir(f'./tmp/{folder}')\n",
    "\n",
    "    with open(f'tmp/{folder}/X_train.pickle', 'wb') as f: pickle.dump(X_train, f)\n",
    "    with open(f'tmp/{folder}/X_test.pickle', 'wb') as f: pickle.dump(X_test, f)\n",
    "    with open(f'tmp/{folder}/y_train.pickle', 'wb') as f: pickle.dump(y_train, f)\n",
    "    with open(f'tmp/{folder}/y_test.pickle', 'wb') as f: pickle.dump(y_test, f)\n",
    "    with open(f'tmp/{folder}/train_indexes.pickle', 'wb') as f: pickle.dump(train_indexes, f)\n",
    "    with open(f'tmp/{folder}/test_indexes.pickle', 'wb') as f: pickle.dump(test_indexes, f)\n",
    "\n",
    "    with open(rf'./tmp/{folder}/metadata.yaml', 'w') as file: yaml.dump(settings, file)\n",
    "\n",
    "    print(f'Saved files to \"./tmp/{folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 20,\n",
    "    'HZ': 20,\n",
    "    'STEP_SIZE': 20,\n",
    "    'TEST_PROPORTION': 0.2,\n",
    "    'AGGREGATION': \"normal\",\n",
    "    'FEATURES': \"all (mean & std)\"\n",
    "}\n",
    "\n",
    "directory = f\"sequential_{settings['HZ']}hz_{settings['MOVING_WINDOW_SIZE']}sec\"\n",
    "print(directory)\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indexes, test_indexes = preprocess_sequential_2(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION']\n",
    ")\n",
    "\n",
    "save_sequential_preprocessing(X_train, X_test, y_train, y_test, train_indexes, test_indexes, directory, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function that gets dataset and and yields tensors with a batch of sequences\n",
    "# this function is used directly within keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(data, moving_window_seconds, hz, step_size):\n",
    "\n",
    "    # split into x and y\n",
    "    labels = data['y']\n",
    "    data = data.drop(columns=['y'])\n",
    "\n",
    "    # to numpy\n",
    "    labels = labels.to_numpy()\n",
    "    data = data.to_numpy()\n",
    "\n",
    "    # select starter rows\n",
    "    max_index = len(data) -1\n",
    "    min_index = 0\n",
    "    lookback = moving_window_seconds * hz\n",
    "\n",
    "    i = min_index + lookback\n",
    "\n",
    "    rows = np.arange(i, max_index, step_size)\n",
    "    # print(rows)\n",
    "\n",
    "    # create empty data structures\n",
    "    samples = np.zeros((len(rows), lookback, data.shape[-1]))\n",
    "    targets = []\n",
    "\n",
    "    print(samples.shape)\n",
    "\n",
    "    # add data for every starter row to data structures\n",
    "    for j, row in enumerate(rows):\n",
    "        indices = range(row - lookback, row)\n",
    "        samples[j] = data[indices]\n",
    "        targets.append(labels[row])\n",
    "\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensor(total, new_part):\n",
    "    if total is not None:\n",
    "        total = np.concatenate((total, new_part))\n",
    "    else:\n",
    "        total = new_part\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequential(moving_window_seconds, hz, step_size, test_proportion = 0.2):\n",
    "    # create empty data frames\n",
    "    x_train = None\n",
    "    x_test = None\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    for file, user, activity in zip(meta['file'], meta['user'], meta['activity']):\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "            df = df.drop(columns=['datetime'])\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, df.shape)\n",
    "\n",
    "        # create synthetic features\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds\n",
    "        )\n",
    "\n",
    "        # aggregate data points (try moving average) transform to mean, sd, ...\n",
    "        for i, (v_train, v_test) in enumerate(zip(my_train_files, my_test_files)):\n",
    "\n",
    "            x_train_next, y_train_next = create_tensors(v_train, moving_window_seconds, hz, step_size)\n",
    "            x_test_next, y_test_next = create_tensors(v_test, moving_window_seconds, hz, step_size)\n",
    "\n",
    "            x_train = concat_tensor(x_train, x_train_next)\n",
    "            x_test = concat_tensor(x_test, x_test_next)\n",
    "            y_train += y_train_next\n",
    "            y_test += y_test_next\n",
    "\n",
    "    # one hot encode labels\n",
    "    y_train = one_hot_encode(y_train)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "\n",
    "    # normalize data by training aggregates\n",
    "    mean = x_train.mean(axis=0).mean(axis=0)\n",
    "    x_train -= mean\n",
    "    x_test -= mean\n",
    "    std = x_train.std(axis=0).std(axis=0)\n",
    "    x_train /= std\n",
    "    x_test /= std\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_sequential(20, 20, 20, test_proportion = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(moving_window_seconds, hz, step_size, agg_func ,test_proportion = 0.2):\n",
    "    '''\n",
    "    agg_func: aggregate function to apply eg add_moving_window or add_moving_window_2\n",
    "    '''\n",
    "    # create empty data frames\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    for file, user, activity in zip(meta['file'], meta['user'], meta['activity']):\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, df.shape)\n",
    "\n",
    "        # create synthetic features\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds\n",
    "        )\n",
    "\n",
    "        # print(f'Train: {[len(i) for i in my_train_files]}')\n",
    "        # print(f'Test: {[len(i) for i in my_test_files]}')\n",
    "\n",
    "        # aggregate data points (try moving average) transform to mean, sd, ...\n",
    "        for i, (v_train, v_test) in enumerate(zip(my_train_files, my_test_files)):\n",
    "            # i.reset_index(drop = True)\n",
    "            v_train = agg_func(\n",
    "                v_train, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "            my_train_files[i] = v_train\n",
    "\n",
    "            v_test = agg_func(\n",
    "                v_test, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "            my_test_files[i] = v_test\n",
    "\n",
    "        # print(f'Train: {[len(i) for i in my_train_files]}')\n",
    "        # print(f'Test: {[len(i) for i in my_test_files]}')\n",
    "\n",
    "        # append to train and test\n",
    "        train = aggregate_files(my_train_files, train)\n",
    "        test = aggregate_files(my_test_files, test)\n",
    "\n",
    "    # X - y split for train and test data, shuffle data!?\n",
    "    y_train = train['y'].to_frame()\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_test = test['y'].to_frame()\n",
    "    X_test = test.drop(columns=['y'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessing(X_train, X_test, y_train, y_test, folder: str, settings: str, file_type: str = 'parquet'):\n",
    "    if not os.path.exists(f'./tmp/{folder}'):\n",
    "        os.mkdir(f'./tmp/{folder}')\n",
    "\n",
    "    if file_type == 'parquet':\n",
    "        X_train.to_parquet(f'tmp/{folder}/X_train.parquet')\n",
    "        X_test.to_parquet(f'tmp/{folder}/X_test.parquet')\n",
    "        y_train.to_parquet(f'tmp/{folder}/y_train.parquet')\n",
    "        y_test.to_parquet(f'tmp/{folder}/y_test.parquet')\n",
    "    elif file_type == 'pickle':\n",
    "        with open(f'tmp/{folder}/X_train.pickle', 'wb') as f: pickle.dump(X_train, f)\n",
    "        with open(f'tmp/{folder}/X_test.pickle', 'wb') as f: pickle.dump(X_test, f)\n",
    "        with open(f'tmp/{folder}/y_train.pickle', 'wb') as f: pickle.dump(y_train, f)\n",
    "        with open(f'tmp/{folder}/y_test.pickle', 'wb') as f: pickle.dump(y_test, f)\n",
    "\n",
    "    with open(rf'./tmp/{folder}/metadata.yaml', 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    print(f'Saved files to \"./tmp/{folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'basic_20hz_20sec'\n",
    "\n",
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 20,\n",
    "    'HZ': 20,\n",
    "    'STEP_SIZE': 20,\n",
    "    'TEST_PROPORTION': 0.2,\n",
    "    'AGGREGATION': \"normal\",\n",
    "    'FEATURES': \"all (mean & std)\",\n",
    "    'PREPROCESSING': directory\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION']\n",
    ")\n",
    "\n",
    "save_preprocessing(X_train, X_test, y_train, y_test, directory, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 20,\n",
    "    'HZ': 20,\n",
    "    'STEP_SIZE': 20,\n",
    "    'TEST_PROPORTION': 0.2,\n",
    "    'AGGREGATION': \"sequential\",\n",
    "    'FEATURES': \"min, max, EucDist\",\n",
    "    'PREPROCESSING': directory\n",
    "}\n",
    "\n",
    "directory = f\"NN_{settings['HZ']}hz_{settings['MOVING_WINDOW_SIZE']}sec\"\n",
    "print(directory)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_sequential(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION']\n",
    ")\n",
    "\n",
    "save_preprocessing(X_train, X_test, y_train, y_test, directory, settings, file_type='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fhnw-ds-cdl1-sRO1VQ75",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1300d74c582e433fb42e1997eddbde559ffc953ada519ef8cefa887b1cf9492"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
