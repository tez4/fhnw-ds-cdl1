{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import wandb\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from src import (\n",
    "    read_nz_file, read_jg_file, update_meta_data, split_df, aggregate_files, add_moving_window, add_moving_window_2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed data - intermediate step\n",
    "\n",
    "> raw data\n",
    "\n",
    "> clean data\n",
    "\n",
    "> preprocess data : store it in DB (better compared to file format) \n",
    "\n",
    "    - data lake\n",
    "    \n",
    "    or\n",
    "    \n",
    "    - DB model based SQL (Nice to have but not required if we deceide to save the data as a Feather file)\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and clean raw data\n",
    "\n",
    "files from SensorLog iOS app has in total over 70 colums and precision of 12 decimal figures. The output file is over 135 MB, which is too large for GitHub. GitHub restricts the file size, therefore these files from SensorLog must be cleaned.\n",
    "\n",
    "Below summary of steps which is done only for iOS files:\n",
    "\n",
    "- read raw data as csv files\n",
    "- remove unnecessary columns (captured in list 'remove_cols' below)\n",
    "- round to 6 decimal places to reduce the size of files\n",
    "- output dataframe as csv\n",
    "- upload the csv on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Meta data\n",
    "\n",
    "The data on different data files is captured in meta dataframe below:\n",
    "- file name\n",
    "- user (nz or jg)\n",
    "- activity (running/cycling/walking/sitting)\n",
    "- pocket (in which pocket handy was during the activity)\n",
    "- position_x\n",
    "- position_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. View data\n",
    "\n",
    "Sensor activity data is captured from 2 different Apps:\n",
    "- SensorLog (iOS) by user 'nz'\n",
    "- AndrioSensor (Andriod) by user 'jg'\n",
    "\n",
    "**Response**: 'Acivity' with 4 classes: running/walking/cycling/sitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_meta_data()\n",
    "\n",
    "meta = pd.read_csv('data/meta.csv')\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects np array as input\n",
    "def one_hot_encode(l):\n",
    "    for i, v in enumerate(l):\n",
    "        if v == 'sitting':\n",
    "            l[i] = 0\n",
    "        elif v == 'walking':\n",
    "            l[i] = 1\n",
    "        elif v == 'running':\n",
    "            l[i] = 2\n",
    "        elif v == 'cycling':\n",
    "            l[i] = 3\n",
    "\n",
    "    l = to_categorical(np.array(l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an aggregate of all files and output a list with the possible start positions within the dataframe. (lookback must always be contained within a single activity)\n",
    "# saves this preprocessed file and list to the tmp folder\n",
    "def preprocess_sequential(moving_window_seconds, hz, step_size, test_proportion = 0.2, select_train_files = 'all'):\n",
    "\n",
    "    # create empty data frames\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    # create empty lists of start indexes\n",
    "    train_indexes = []\n",
    "    test_indexes = []\n",
    "\n",
    "    all_train_files = []\n",
    "    all_test_files = []\n",
    "\n",
    "    for index, (file, user, activity, position_x) in enumerate(\n",
    "            zip(meta['file'], meta['user'], meta['activity'], meta['position_x'])):\n",
    "\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "            df = df.drop(columns=['datetime'])\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, position_x, df.shape)\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds,\n",
    "            select_train_files = select_train_files, user = user, position_x = position_x, index = index\n",
    "        )\n",
    "\n",
    "        all_train_files += my_train_files\n",
    "        all_test_files += my_test_files\n",
    "\n",
    "    print([len(i) for i in all_train_files])\n",
    "    print([len(i) for i in all_test_files])\n",
    "\n",
    "    lookback = hz * moving_window_seconds\n",
    "    # aggregate data points (try moving average) transform to mean, sd, ...\n",
    "    for i, v_train in enumerate(all_train_files):\n",
    "\n",
    "        # save possible start indexes for training and test sequences\n",
    "        max_train_index = len(v_train) - 1\n",
    "        train_rows = np.arange(lookback, max_train_index, step_size)\n",
    "\n",
    "        # append start indexes of sequences to total lists\n",
    "        train_indexes += list(train_rows + len(train))\n",
    "\n",
    "        # append files to total file\n",
    "        train = pd.concat([train, v_train])\n",
    "\n",
    "    for i, v_test in enumerate(all_test_files):\n",
    "\n",
    "        # save possible start indexes for training and test sequences\n",
    "        max_test_index = len(v_test) - 1\n",
    "        test_rows = np.arange(lookback, max_test_index, step_size)\n",
    "\n",
    "        # append start indexes of sequences to total lists\n",
    "        test_indexes += list(test_rows + len(test))\n",
    "\n",
    "        # append files to total file\n",
    "        test = pd.concat([test, v_test])\n",
    "\n",
    "    # split x and y\n",
    "    cols = [\n",
    "        'v_accelerometer',\n",
    "        'v_gyroscope',\n",
    "        'v_magnetometer',\n",
    "        'v_gravity',\n",
    "        'v_orientation',\n",
    "        'min_accelerometer',\n",
    "        'max_accelerometer',\n",
    "        'min_gyroscope',\n",
    "        'max_gyroscope',\n",
    "        'min_magnetometer',\n",
    "        'max_magnetometer',\n",
    "        'min_gravity',\n",
    "        'max_gravity',\n",
    "        'min_orientation',\n",
    "        'max_orientation'\n",
    "    ]\n",
    "\n",
    "    y_train = train['y']\n",
    "    # x_train = train.drop(columns=['y', 'time_since_start(ms)'])\n",
    "    x_train = train[cols]\n",
    "\n",
    "    y_test = test['y']\n",
    "    # x_test = test.drop(columns=['y', 'time_since_start(ms)'])\n",
    "    x_test = test[cols]\n",
    "\n",
    "    # normalize data by training aggregates\n",
    "    mean = x_train.mean(axis=0)\n",
    "    x_train -= mean\n",
    "    x_test -= mean\n",
    "    std = x_train.std(axis=0)\n",
    "    x_train /= std\n",
    "    x_test /= std\n",
    "\n",
    "    # one hot encode labels\n",
    "    y_train = one_hot_encode(np.array(y_train))\n",
    "    y_test = one_hot_encode(np.array(y_test))\n",
    "\n",
    "    # shuffle the list of indexes\n",
    "    random.shuffle(train_indexes)\n",
    "    random.shuffle(test_indexes)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, train_indexes, test_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sequential_preprocessing(\n",
    "        X_train, X_test, y_train, y_test, train_indexes, test_indexes, folder: str, settings: str):\n",
    "\n",
    "    if not os.path.exists(f'./tmp/{folder}'):\n",
    "        os.mkdir(f'./tmp/{folder}')\n",
    "\n",
    "    with open(f'tmp/{folder}/X_train.pickle', 'wb') as f: pickle.dump(X_train, f)\n",
    "    with open(f'tmp/{folder}/X_test.pickle', 'wb') as f: pickle.dump(X_test, f)\n",
    "    with open(f'tmp/{folder}/y_train.pickle', 'wb') as f: pickle.dump(y_train, f)\n",
    "    with open(f'tmp/{folder}/y_test.pickle', 'wb') as f: pickle.dump(y_test, f)\n",
    "    with open(f'tmp/{folder}/train_indexes.pickle', 'wb') as f: pickle.dump(train_indexes, f)\n",
    "    with open(f'tmp/{folder}/test_indexes.pickle', 'wb') as f: pickle.dump(test_indexes, f)\n",
    "\n",
    "    with open(rf'./tmp/{folder}/metadata.yaml', 'w') as file: yaml.dump(settings, file)\n",
    "\n",
    "    print(f'Saved files to \"./tmp/{folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for moving_window_size in [1, 2]:\n",
    "#     for hz in [1, 2, 5, 10, 20]:\n",
    "\n",
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 2,\n",
    "    'HZ': 5,\n",
    "    'STEP_SIZE': 5,\n",
    "    'TEST_PROPORTION': 0.5,\n",
    "    'AGGREGATION': \"sequential\",\n",
    "    'FEATURES': \"min, max, EucDist\"\n",
    "}\n",
    "\n",
    "directory = f\"sequential_index_{settings['HZ']}hz_{settings['MOVING_WINDOW_SIZE']}sec\"\n",
    "print(directory)\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indexes, test_indexes = preprocess_sequential(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION'],\n",
    "    select_train_files = 'index'\n",
    ")\n",
    "\n",
    "save_sequential_preprocessing(X_train, X_test, y_train, y_test, train_indexes, test_indexes, directory, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(moving_window_seconds, hz, step_size, agg_func, test_proportion = 0.2, select_train_files = 'all'):\n",
    "    '''\n",
    "    agg_func: aggregate function to apply eg add_moving_window or add_moving_window_2\n",
    "    '''\n",
    "    # create empty data frames\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    all_train_files = []\n",
    "    all_test_files = []\n",
    "\n",
    "    for index, (file, user, activity, position_x) in enumerate(\n",
    "            zip(meta['file'], meta['user'], meta['activity'], meta['position_x'])):\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, position_x, df.shape)\n",
    "\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds,\n",
    "            select_train_files = select_train_files, user = user, position_x = position_x, index = index\n",
    "        )\n",
    "\n",
    "        print(f'Train: {len(my_train_files)}, Test: {len(my_test_files)}')\n",
    "\n",
    "        all_train_files += my_train_files\n",
    "        all_test_files += my_test_files\n",
    "\n",
    "    print(f'Train: {[len(i) for i in all_train_files]}')\n",
    "    print(f'Test: {[len(i) for i in all_test_files]}')\n",
    "\n",
    "    # aggregate every file in training set\n",
    "    for i, v_train in enumerate(all_train_files):\n",
    "        if agg_func == 'add_moving_window_2':\n",
    "            v_train = add_moving_window_2(\n",
    "                v_train, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "\n",
    "        elif agg_func == 'add_moving_window':\n",
    "            v_train = add_moving_window(\n",
    "                v_train, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "\n",
    "        train = pd.concat([train, v_train])\n",
    "\n",
    "    # aggregate every file in test set\n",
    "    for i, v_test in enumerate(all_test_files):\n",
    "        if agg_func == 'add_moving_window_2':\n",
    "            v_test = add_moving_window_2(\n",
    "                v_test, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "        elif agg_func == 'add_moving_window':\n",
    "\n",
    "            v_test = add_moving_window(\n",
    "                v_test, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "\n",
    "        test = pd.concat([test, v_test])\n",
    "\n",
    "\n",
    "    print(f'Train length: {len(train)}')\n",
    "    print(f'Test length: {len(test)}')\n",
    "\n",
    "    # X - y split for train and test data, shuffle data!?\n",
    "    y_train = train['y'].to_frame()\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_test = test['y'].to_frame()\n",
    "    X_test = test.drop(columns=['y'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessing(X_train, X_test, y_train, y_test, folder: str, settings: str, file_type: str = 'parquet'):\n",
    "    if not os.path.exists(f'./tmp/{folder}'):\n",
    "        os.mkdir(f'./tmp/{folder}')\n",
    "\n",
    "    if file_type == 'parquet':\n",
    "        X_train.to_parquet(f'tmp/{folder}/X_train.parquet')\n",
    "        X_test.to_parquet(f'tmp/{folder}/X_test.parquet')\n",
    "        y_train.to_parquet(f'tmp/{folder}/y_train.parquet')\n",
    "        y_test.to_parquet(f'tmp/{folder}/y_test.parquet')\n",
    "    elif file_type == 'pickle':\n",
    "        with open(f'tmp/{folder}/X_train.pickle', 'wb') as f: pickle.dump(X_train, f)\n",
    "        with open(f'tmp/{folder}/X_test.pickle', 'wb') as f: pickle.dump(X_test, f)\n",
    "        with open(f'tmp/{folder}/y_train.pickle', 'wb') as f: pickle.dump(y_train, f)\n",
    "        with open(f'tmp/{folder}/y_test.pickle', 'wb') as f: pickle.dump(y_test, f)\n",
    "\n",
    "    with open(rf'./tmp/{folder}/metadata.yaml', 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    print(f'Saved files to \"./tmp/{folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'dimension_independent_5hz_2sec'\n",
    "\n",
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 2,\n",
    "    'HZ': 5,\n",
    "    'STEP_SIZE': 5,\n",
    "    'TEST_PROPORTION': 0.2,\n",
    "    'AGGREGATION': \"normal\",\n",
    "    'FEATURES': \"min, max, EucDist (mean & std)\",\n",
    "    'PREPROCESSING': directory\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION'],\n",
    "    agg_func = 'add_moving_window_2',\n",
    "    select_train_files='user' # 'all', 'index', 'position_x'\n",
    ")\n",
    "\n",
    "save_preprocessing(X_train, X_test, y_train, y_test, directory, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fhnw-ds-cdl1-sRO1VQ75",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1300d74c582e433fb42e1997eddbde559ffc953ada519ef8cefa887b1cf9492"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
