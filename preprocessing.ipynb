{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import wandb\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from src import read_nz_file, read_jg_file, update_meta_data, split_df, aggregate_files, add_moving_window\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed data - intermediate step\n",
    "\n",
    "> raw data\n",
    "\n",
    "> clean data\n",
    "\n",
    "> preprocess data : store it in DB (better compared to file format) \n",
    "\n",
    "    - data lake\n",
    "    \n",
    "    or\n",
    "    \n",
    "    - DB model based SQL (Nice to have but not required if we deceide to save the data as a Feather file)\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and clean raw data\n",
    "\n",
    "files from SensorLog iOS app has in total over 70 colums and precision of 12 decimal figures. The output file is over 135 MB, which is too large for GitHub. GitHub restricts the file size, therefore these files from SensorLog must be cleaned.\n",
    "\n",
    "Below summary of steps which is done only for iOS files:\n",
    "\n",
    "- read raw data as csv files\n",
    "- remove unnecessary columns (captured in list 'remove_cols' below)\n",
    "- round to 6 decimal places to reduce the size of files\n",
    "- output dataframe as csv\n",
    "- upload the csv on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Meta data\n",
    "\n",
    "The data on different data files is captured in meta dataframe below:\n",
    "- file name\n",
    "- user (nz or jg)\n",
    "- activity (running/cycling/walking/sitting)\n",
    "- pocket (in which pocket handy was during the activity)\n",
    "- position_x\n",
    "- position_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. View data\n",
    "\n",
    "Sensor activity data is captured from 2 different Apps:\n",
    "- SensorLog (iOS) by user 'nz'\n",
    "- AndrioSensor (Andriod) by user 'jg'\n",
    "\n",
    "**Response**: 'Acivity' with 4 classes: running/walking/cycling/sitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_meta_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('data/meta.csv')\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(data, moving_window_seconds, hz, step_size):\n",
    "\n",
    "    # split into x and y\n",
    "    labels = data['y']\n",
    "    data = data.drop(columns=['y'])\n",
    "\n",
    "    # to numpy\n",
    "    labels = labels.to_numpy()\n",
    "    data = data.to_numpy()\n",
    "\n",
    "    # select starter rows\n",
    "    max_index = len(data) -1\n",
    "    min_index = 0\n",
    "    lookback = moving_window_seconds * hz\n",
    "\n",
    "    i = min_index + lookback\n",
    "\n",
    "    rows = np.arange(i, max_index, step_size)\n",
    "    # print(rows)\n",
    "\n",
    "    # create empty data structures\n",
    "    samples = np.zeros((len(rows), lookback, data.shape[-1]))\n",
    "    targets = []\n",
    "\n",
    "    print(samples.shape)\n",
    "\n",
    "    # add data for every starter row to data structures\n",
    "    for j, row in enumerate(rows):\n",
    "        indices = range(row - lookback, row)\n",
    "        samples[j] = data[indices]\n",
    "        targets.append(labels[row])\n",
    "\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensor(total, new_part):\n",
    "    if total is not None:\n",
    "        total = np.concatenate((total, new_part))\n",
    "    else:\n",
    "        total = new_part\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(l):\n",
    "    for i, v in enumerate(l):\n",
    "        if v == 'sitting':\n",
    "            l[i] = 0\n",
    "        elif v == 'walking':\n",
    "            l[i] = 1\n",
    "        elif v == 'running':\n",
    "            l[i] = 2\n",
    "        elif v == 'cycling':\n",
    "            l[i] = 3\n",
    "\n",
    "    l = to_categorical(np.array(l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequential(moving_window_seconds, hz, step_size, test_proportion = 0.2):\n",
    "    # create empty data frames\n",
    "    x_train = None\n",
    "    x_test = None\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    for file, user, activity in zip(meta['file'], meta['user'], meta['activity']):\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "            df = df.drop(columns=['datetime'])\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, df.shape)\n",
    "\n",
    "        # create synthetic features\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds\n",
    "        )\n",
    "\n",
    "        # aggregate data points (try moving average) transform to mean, sd, ...\n",
    "        for i, (v_train, v_test) in enumerate(zip(my_train_files, my_test_files)):\n",
    "\n",
    "            x_train_next, y_train_next = create_tensors(v_train, moving_window_seconds, hz, step_size)\n",
    "            x_test_next, y_test_next = create_tensors(v_test, moving_window_seconds, hz, step_size)\n",
    "\n",
    "            x_train = concat_tensor(x_train, x_train_next)\n",
    "            x_test = concat_tensor(x_test, x_test_next)\n",
    "            y_train += y_train_next\n",
    "            y_test += y_test_next\n",
    "\n",
    "    # one hot encode labels\n",
    "    y_train = one_hot_encode(y_train)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "\n",
    "    # normalize data by training aggregates\n",
    "    mean = x_train.mean(axis=0).mean(axis=0)\n",
    "    x_train -= mean\n",
    "    x_test -= mean\n",
    "    std = x_train.std(axis=0).std(axis=0)\n",
    "    x_train /= std\n",
    "    x_test /= std\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_sequential(20, 20, 20, test_proportion = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(moving_window_seconds, hz, step_size, agg_func ,test_proportion = 0.2):\n",
    "    '''\n",
    "    agg_func: aggregate function to apply eg add_moving_window or add_moving_window_2\n",
    "    '''\n",
    "    # create empty data frames\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    for file, user, activity in zip(meta['file'], meta['user'], meta['activity']):\n",
    "        if user == 'nz':\n",
    "            df = read_nz_file(file, activity)\n",
    "\n",
    "        elif user == 'jg':\n",
    "            df = read_jg_file(file, activity)\n",
    "\n",
    "        print(file, user, activity, df.shape)\n",
    "\n",
    "        # create synthetic features\n",
    "\n",
    "        # split into train-test\n",
    "        my_train_files, my_test_files = split_df(\n",
    "            df, hz = hz, test_proportion = test_proportion, moving_window_size = moving_window_seconds\n",
    "        )\n",
    "\n",
    "        # print(f'Train: {[len(i) for i in my_train_files]}')\n",
    "        # print(f'Test: {[len(i) for i in my_test_files]}')\n",
    "\n",
    "        # aggregate data points (try moving average) transform to mean, sd, ...\n",
    "        for i, (v_train, v_test) in enumerate(zip(my_train_files, my_test_files)):\n",
    "            # i.reset_index(drop = True)\n",
    "            v_train = agg_func(\n",
    "                v_train, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "            my_train_files[i] = v_train\n",
    "\n",
    "            v_test = agg_func(\n",
    "                v_test, hz_old_data = hz, seconds = moving_window_seconds, step_size = step_size\n",
    "            )\n",
    "            my_test_files[i] = v_test\n",
    "\n",
    "        # print(f'Train: {[len(i) for i in my_train_files]}')\n",
    "        # print(f'Test: {[len(i) for i in my_test_files]}')\n",
    "\n",
    "        # append to train and test\n",
    "        train = aggregate_files(my_train_files, train)\n",
    "        test = aggregate_files(my_test_files, test)\n",
    "\n",
    "    # X - y split for train and test data, shuffle data!?\n",
    "    y_train = train['y'].to_frame()\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_test = test['y'].to_frame()\n",
    "    X_test = test.drop(columns=['y'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessing(X_train, X_test, y_train, y_test, folder: str, settings: str, file_type: str = 'parquet'):\n",
    "    if not os.path.exists(f'./tmp/{folder}'):\n",
    "        os.mkdir(f'./tmp/{folder}')\n",
    "\n",
    "    if file_type == 'parquet':\n",
    "        X_train.to_parquet(f'tmp/{folder}/X_train.parquet')\n",
    "        X_test.to_parquet(f'tmp/{folder}/X_test.parquet')\n",
    "        y_train.to_parquet(f'tmp/{folder}/y_train.parquet')\n",
    "        y_test.to_parquet(f'tmp/{folder}/y_test.parquet')\n",
    "    elif file_type == 'pickle':\n",
    "        with open(f'tmp/{folder}/X_train.pickle', 'wb') as f: pickle.dump(X_train, f)\n",
    "        with open(f'tmp/{folder}/X_test.pickle', 'wb') as f: pickle.dump(X_test, f)\n",
    "        with open(f'tmp/{folder}/y_train.pickle', 'wb') as f: pickle.dump(y_train, f)\n",
    "        with open(f'tmp/{folder}/y_test.pickle', 'wb') as f: pickle.dump(y_test, f)\n",
    "\n",
    "    with open(rf'./tmp/{folder}/metadata.yaml', 'w') as file:\n",
    "        yaml.dump(settings, file)\n",
    "\n",
    "    print(f'Saved files to \"./tmp/{folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'basic_20hz_20sec'\n",
    "\n",
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 20,\n",
    "    'HZ': 20,\n",
    "    'STEP_SIZE': 20,\n",
    "    'TEST_PROPORTION': 0.2,\n",
    "    'AGGREGATION': \"normal\",\n",
    "    'FEATURES': \"all (mean & std)\",\n",
    "    'PREPROCESSING': directory\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION']\n",
    ")\n",
    "\n",
    "save_preprocessing(X_train, X_test, y_train, y_test, directory, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'MOVING_WINDOW_SIZE': 20,\n",
    "    'HZ': 20,\n",
    "    'STEP_SIZE': 20,\n",
    "    'TEST_PROPORTION': 0.2,\n",
    "    'AGGREGATION': \"sequential\",\n",
    "    'FEATURES': \"min, max, EucDist\",\n",
    "    'PREPROCESSING': directory\n",
    "}\n",
    "\n",
    "directory = f\"NN_{settings['HZ']}hz_{settings['MOVING_WINDOW_SIZE']}sec\"\n",
    "print(directory)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_sequential(\n",
    "    moving_window_seconds = settings['MOVING_WINDOW_SIZE'],\n",
    "    hz = settings['HZ'],\n",
    "    step_size = settings['STEP_SIZE'],\n",
    "    test_proportion = settings['TEST_PROPORTION']\n",
    ")\n",
    "\n",
    "save_preprocessing(X_train, X_test, y_train, y_test, directory, settings, file_type='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1300d74c582e433fb42e1997eddbde559ffc953ada519ef8cefa887b1cf9492"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
